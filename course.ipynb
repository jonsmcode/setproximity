{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6246c9c1-a703-4a47-94fe-1c030bfa8a08",
   "metadata": {},
   "source": [
    "# Nanocourse - Genomic analysis using sketching techniques\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5627c9-8906-4a39-a9d0-444662d47c58",
   "metadata": {},
   "source": [
    "\n",
    "## Sketching\n",
    "\n",
    "A *data sketch* of data $X$ is the output of a (randomized) function $f$ s.t.:\n",
    " - $|f(X)| \\subseteq o(|X|)$\n",
    " - $f$ perserves some properties of $X$ e.g. approximation of the number of distinct elements\n",
    " - $f$ preserves certain similarity measures e.g. number of shared elements\n",
    " - $f(X)$ allows to be updated efficiently\n",
    "\n",
    "\n",
    "## Counting distinct elements of a set\n",
    "\n",
    "\n",
    "\n",
    "### Naive solution\n",
    "\n",
    "Using a Hashmap or a Bitvector\n",
    "\n",
    "**TODO:** Implement naive_counting() in naive_couting0.cpp\n",
    "\n",
    "Test your implementation by running\n",
    "```\n",
    "./build/source/naivecounting0 data/ecoli1_k31_ust.fa.gz\n",
    "```\n",
    "The output should be\n",
    "```\n",
    "Distinct kmers: 4877400\n",
    "```\n",
    "How much memory consumption do you expect?\n",
    "\n",
    "Let us check that in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa38cc-6d35-4615-be71-4dc7a238fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from evaluate.track import track_memory_and_runtime\n",
    "\n",
    "file = \"data/ecoli1_k31_ust.fa.gz\"\n",
    "file_size = os.path.getsize(file)/(1024*1024)\n",
    "_,memory,_ = track_memory_and_runtime(\"build/source/naivecounting0\", file)\n",
    "print(\"File size: %.1fMB\" % file_size)\n",
    "print(\"Memory consumption: %.1fMB\" % memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be418691-7e3c-4236-b7bf-59b5d544034c",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "- space consumption linear w.r.t. number distinct elements\n",
    "- impractical for big data\n",
    "- impractical in online setting (stream) as number of distinct elements grows exponentially ($4^k$ distinct k-mers in DNA)\n",
    "\n",
    "-> approximate/probabilistic counting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc5799-42b5-4184-b1f8-68c9b99bd3e0",
   "metadata": {},
   "source": [
    "### Flajolet-Martinâ€™s algorithm\n",
    "\n",
    "#### Recall:\n",
    "\n",
    "Let $\\mathcal{M}$ be a multiset of uniformly distributed random numbers.\n",
    " - The cardinality of $\\mathcal{M}$ can be estimated by the maximum number of leading zeros in the binary representation of each number in $\\mathcal{M}$.\n",
    " - If max leading zeros is $l$, one exepcts $2^l$ distinct elements\n",
    "(the probability of observing a binary encoded number beginning with $k$ zeroes followed by a one is $1/2^{(k+1)}$ ).\n",
    "\n",
    "#### Algorithm:\n",
    "\n",
    "- Map each element $x$ to hash $h(x)$\n",
    "- remember the maximum number $l$ of leading 0-bits seen in any $h(x)$\n",
    "- estimate cardinality by $2^l$\n",
    "\n",
    "<!-- % $\\Psi \\approx 0.77351$ is a normalization constant. -->\n",
    "\n",
    "**TODO**\n",
    "- Implement flajolet_martin() in flajolet_martin0.cpp\n",
    "- Test it by running\n",
    "```\n",
    "./build/source/flajoletmartin0 data/ecoli1_k31_ust.fa.gz\n",
    "```\n",
    "Let us compare run time, space consumption, and accuracy to exact algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe88246-6d2f-4c16-bfca-57cc5ad394e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from evaluate.plots import plot_performance, plot_accuracy_FM, plot_accuracies\n",
    "\n",
    "files = [\"data/ecoli1_k31_ust.fa.gz\", \"data/ecoli2_k31_ust.fa.gz\",\n",
    "         \"data/yeast.k31.unitigs.fa.ust.fa.gz\", \"data/salmonella_100_k31_ust.fa.gz\"]\n",
    "\n",
    "naive_results, fm_results = [], []\n",
    "for file in files:\n",
    "    naive_results.append(track_memory_and_runtime(\"build/source/naivecounting0\", file))\n",
    "    fm_results.append(track_memory_and_runtime(\"build/source/flajoletmartin0\", file))\n",
    "\n",
    "file_sizes = [os.path.getsize(file) for file in files]\n",
    "results = np.array([naive_results, fm_results])\n",
    "algorithms = [\"Naive\", \"Flajolet Martin\"]\n",
    "\n",
    "plot_performance(results, algorithms, file_sizes)\n",
    "# plot_accuracy_FM(results, algorithms)\n",
    "errors=np.abs(results[1,:,2] - results[0,:,2])/results[0,:,2]\n",
    "print(\"Error: %.1f+-%.1f%%\" % (np.mean(errors)*100, np.std(errors)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb2302c-8c43-4d0d-8fb7-47bbc1c6cd2d",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- Large variance! How could you compensate that?\n",
    "- Less space consumption: hash $h(x) \\rightarrow [0,L]$ requires $\\log(L) \\approx \\log(n)$ space for $n$ distinct elements i.e. exponential less space w.r.t. sequence length $n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dc1840-0295-4823-9a0e-2e53474fc536",
   "metadata": {},
   "source": [
    "### FM+\n",
    "\n",
    "#### Improving the variance\n",
    "\n",
    "- Compute the mean of $s$ independent instances of FM\n",
    "- What improvement do you expect?\n",
    "- How could you yield $s$ independent instances i.e. different hashfunctions?\n",
    "\n",
    "**TODO:** Implement fmplus() in fmplus0.cpp \n",
    "You can use multi_hash(x,s) to obtain $s$ independent hashs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e6eaf9-1481-4d86-a2f9-a19f56fcffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmplus_results = []\n",
    "for file in files:\n",
    "    fmplus_results.append(track_memory_and_runtime(\"build/source/fmplus0\", file))\n",
    "\n",
    "results = np.array([naive_results, fmplus_results])\n",
    "algorithms = [\"Flajolet Martin\", \"Boosted Flajolet Martin\"]\n",
    "\n",
    "plot_accuracy_FM(results, algorithms)\n",
    "errors=np.abs(results[1,:,2] - results[0,:,2])/results[0,:,2]\n",
    "print(\"Error: %.1f+-%.1f%%\" % (np.mean(errors)*100, np.std(errors)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62459981-2c42-4b8e-8ac6-54975ba51432",
   "metadata": {},
   "source": [
    "### HyperLogLog\n",
    "\n",
    "#### Refinement:\n",
    "- split $\\mathcal{M}$ into $m$ subsets\n",
    "- estimate cardinalities of subsets\n",
    "- return harmonic mean\n",
    "\n",
    "#### Algorithm:\n",
    "Let $\\Omega$ be the domain of hashfunction $h$ and $k$ be in $\\{1,\\ldots,|\\Omega|\\}$.\n",
    "\n",
    "- allocate space for $2^k$ \"registers\" $M$ to count the maximum observed number of leading zeros\n",
    "- for each element $x$ update the maximum leading zeros of the bits beginnning at $i$ of $h(x)$ of register $M[i]$ with $i$ being the number encoded by the first $k$ bits of $h(x)$\n",
    "\n",
    "\n",
    "The normalized version of the harmonic mean is the estimate\n",
    "\n",
    "$$E:=\\frac{\\alpha_m m^2}{\\sum_{j=1}^m 2^{-M(j)}}.$$\n",
    "\n",
    "for $m$ subsets $M(i)$ and normalization constant $\\alpha_m \\approx 0.7$.\n",
    "\n",
    "**TODO:** Implement hyperloglog() in hyperloglog0.hpp\n",
    "\n",
    "Let us compare run time, space consumption, and accuracy to FM+ and the exact algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4620e016-5b36-49cc-9ef7-aba9832d15b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_results, fm_results, fmplus_results, hll_results = [], [], [], []\n",
    "for file in files:\n",
    "    naive_results.append(track_memory_and_runtime(\"build/source/naivecounting0\", file))\n",
    "    # fm_results.append(track_memory_and_runtime(\"build/source/flajoletmartin1\", file))\n",
    "    fmplus_results.append(track_memory_and_runtime(\"build/source/fmplus0\", file))\n",
    "    hll_results.append(track_memory_and_runtime(\"build/source/hyperloglog0\", file))\n",
    "\n",
    "file_sizes = [os.path.getsize(file) for file in files]\n",
    "results = np.array([naive_results, fmplus_results, hll_results])\n",
    "algorithms = [\"Naive\", \"FM+\", \"HLL\"]\n",
    "\n",
    "# plot_performance(results, algorithms, file_sizes)\n",
    "plot_accuracies(results, algorithms)\n",
    "# error_fm=np.abs(results[1,:,2] - results[0,:,2])/results[0,:,2]\n",
    "error_fmplus=np.abs(results[1,:,2] - results[0,:,2])/results[0,:,2]\n",
    "error_hll=np.abs(results[2,:,2] - results[0,:,2])/results[0,:,2]\n",
    "print(\"Total Runtime Hashtable: %.1fs, FM+: %.1fs, HLL: %.1fs\" % (np.sum(results[0,:,0]), np.sum(results[1,:,0]), np.sum(results[2,:,0])))\n",
    "print(\"Maximum Memory Hashtable: %.1fMb, FM+: %.1fMb, HLL: %.1fMb\" % (np.max(results[0,:,1]), np.max(results[1,:,1]), np.max(results[2,:,1])))\n",
    "# print(\"Error Flajolet Martin: %.1f+-%.1f%%\" % (np.mean(error_fm)*100, np.std(error_fm)*100))\n",
    "print(\"Error Flajolet Martin+: %.1f+-%.1f%%\" % (np.mean(error_fmplus)*100, np.std(error_fmplus)*100))\n",
    "print(\"Error HyperLogLog: %.1f+-%.1f%%\" % (np.mean(error_hll)*100, np.std(error_hll)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a92454-d268-4782-8f28-e70b8fe0c0af",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "- Smaller variance\n",
    "- Less space consumption: $O(\\log \\log n)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a456e4-588e-405e-bc15-adddfc98632c",
   "metadata": {},
   "source": [
    "## Set Similarity\n",
    "\n",
    "(Dis-)similarity of two sets $A$ and $B$ can be measured with Jaccard similarity\n",
    "$$\n",
    "J(A,B) := \\frac{|A \\cap B|}{|A \\cup B|}\n",
    "$$\n",
    "\n",
    "\n",
    "### Naive Algorithm\n",
    "\n",
    "Hashmap or bitvector\n",
    "\n",
    "**TODO** Implement jaccard_similarity() in jaccard0.cpp\n",
    "\n",
    "Test it by running\n",
    "```\n",
    "./build/source/jaccard0\n",
    "```\n",
    "\n",
    "### Observations\n",
    "- space consumption linear w.r.t. total number of distinct elements\n",
    "- comparing $n$ sets requires $O(n^2)$ pairwise comarisons.\n",
    "- keeping hashmaps of all sets of size $O(m)$ in memory for faster comparison costs $O(n m)$ space.\n",
    "\n",
    "Impractical for big data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df23e3a-5bce-4fda-8b0f-2da86e2a3dcf",
   "metadata": {},
   "source": [
    "### Naive Improvement\n",
    "\n",
    "Randomly sample $p$ percent elements per set and compute Jaccard similarity for them.\n",
    "\n",
    "**TODO:** Implement jaccard_similarities() in jaccard_sample0.cpp\n",
    "\n",
    "Let us compare run time, space consumption, and accuracy to the exact algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20091f7a-37e2-43be-bd7d-de3b883a6400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate.track import track_memory_and_runtime_similarity\n",
    "time_jaccard, mem_jaccard, jaccard = track_memory_and_runtime_similarity(\"build/source/jaccard0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9764eea7-c6ce-4b1a-b2ca-5a41dba0bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = [0.1, 0.25, 0.5, 0.75]\n",
    "results = []\n",
    "for prob in probabilities:\n",
    "    results.append(track_memory_and_runtime_similarity(\"build/source/jaccard_sample0\", prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9133843c-ec7f-41ca-92ca-ed942aabdc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from evaluate.plots import plot_accuracy_runtime\n",
    "\n",
    "indices = np.triu_indices(jaccard.shape[0], k=1)\n",
    "errors = np.empty((len(results), jaccard[indices].shape[0]))\n",
    "runtimes = np.empty(len(results))\n",
    "for i, (time, mem, res) in enumerate(results):\n",
    "    errors[i] = np.mean(np.abs(res[indices]-jaccard[indices])/jaccard[indices])\n",
    "    runtimes[i] = time\n",
    "\n",
    "plot_accuracy_runtime(errors, runtimes, probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c51443-d5a9-404a-8177-bf6b0d82f7c0",
   "metadata": {},
   "source": [
    "### MinHashing\n",
    "\n",
    "Let $A,B$ be two sets, $h$ be a hash function, MinHash $h_{\\min}(A) := \\min \\lbrace h(x) \\mid x \\in A \\rbrace$ and\n",
    "$$\n",
    "J_h(A,B) := \\begin{cases}1, & \\text{if } h_{\\min}(A) = h_{\\min}(B)\\\\ 0 & \\text{otw.}\\end{cases},\n",
    "$$\n",
    "Then $J_h(A,B)$ is a random estimator of $J(A,B)$, i.e.: $E[J_h(A,B)] = J(A,B)$.\n",
    "Scince $J_h$ has too high variance (it is always zero or one), average multiple independent MinHash estimators.\n",
    "\n",
    "#### Algorithm:\n",
    "\n",
    "- sample $h_{\\min}$ from $k$ random permutations of $h$\n",
    "- let $l$ be the number of permutated hash functions with $h_{\\min}(A) = h_{\\min}(B)$\n",
    "- estimate $J(A,B)$ by $l/k$\n",
    "\n",
    "A random permutation of $h$ can be e.g.:\n",
    "$$\n",
    "h_i(x) = a_i x + b_i \\mod p\n",
    "$$\n",
    "for prime number $p$ and random $a_i,b_i \\in \\lbrace 1,\\ldots,p\\rbrace$.\n",
    "Or draw samples from farmhash family using multi_hash() function\n",
    "\n",
    "**TODO:** Implement minhash_similarity() in minhash0.cpp\n",
    "\n",
    "\n",
    "Let us compare run time and accuracy to exact algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce93708-a1dc-47a8-9050-b99ba54f5fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_minhash, mem_minhash, minhash = track_memory_and_runtime_similarity(\"build/source/minhash0\", 512)\n",
    "print(\"Runtime Hashtable: %.1fs, Minhash: %.1fs\" % (time_jaccard, time_minhash))\n",
    "print(\"Memory Hashtable: %.1fMb, Minhash: %.1fMb\" % (mem_jaccard, mem_minhash))\n",
    "error_minhash = np.abs(minhash[indices]-jaccard[indices])/jaccard[indices]\n",
    "print(\"Error: %.1f+-%.1f%%\" % (np.mean(error_minhash)*100, np.std(error_minhash)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81533ef-9a04-4d83-8d58-cce66be62d31",
   "metadata": {},
   "source": [
    " - For any $\\epsilon > 0$ there is a $k \\in O(1/\\epsilon^2)$ s.t. the expected error is at most $\\epsilon$.\n",
    "\n",
    " - How many permutations $k$ do you need to have an expected error at most $.05$?\n",
    "\n",
    "Let us explore the accuracy and run time of MinHash w.r.t. the number of permutations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db596a8-dc13-4c93-9ccb-d34a8b4e40e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate.plots import plot_minhash\n",
    "permutations = np.linspace(100,2500,10)\n",
    "runtimes, errors = [], []\n",
    "for perms in permutations:\n",
    "    time, mem, res = track_memory_and_runtime_similarity(\"build/source/minhash0\", perms)\n",
    "    runtimes.append(time)\n",
    "    errors.append(np.mean(np.abs(res[indices]-jaccard[indices])/jaccard[indices]))\n",
    "plot_minhash(errors, runtimes, permutations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579304d1-d2b1-40cd-b0ac-1934817ee3a0",
   "metadata": {},
   "source": [
    "### FracMinHashing\n",
    "\n",
    "Let $h: \\Omega \\Rightarrow [0,H]$ be a hash function for $H \\in \\mathbb{N}$ and $s \\in [0,1]$ be a scaling factor. Then FracMinHash sketch\n",
    "$$\n",
    "FRAC_{s}(A) := \\{ h(x) \\mid x \\in A \\land h(x) \\leq H s \\}.\n",
    "$$\n",
    "The Containment Index $C(A,B) = \\frac{|A \\cap B|}{|A|}$ is the estimate of\n",
    "$$\n",
    "\\hat{C}_s(A,B) = \\frac{|FRAC_{s}(A) \\cap FRAC_{s}(B)|}{|FRAC_{s}(A)| (1-(1-s)^{|A|})}\n",
    "$$\n",
    "where $1/(1-(1-s)^{|A|})$ compensates the bias.\n",
    "\n",
    "But we do not know $|A|$? Is that a problem?\n",
    "\n",
    "\n",
    "**TODO:** Implement fracminhash_similarities() in fracMinHash.cpp\n",
    "\n",
    "Note that the Jaccard similarity\n",
    "$$\n",
    "J(A,B) = \\frac{C(A,B)|A|}{|A|+|B|-C(A,B)|A|}.\n",
    "$$\n",
    "\n",
    "Let us compare run time and accuracy to MinHash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3138b8c-d4a8-40b9-b960-1b1e72f622d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_fracminhash, mem_fracminhash, fracminhash = track_memory_and_runtime_similarity(\"build/source/fracminhash0\")\n",
    "print(\"Runtime MinHash: %.1fs, FracMinHash: %.1fs\" % (time_minhash, time_fracminhash))\n",
    "error_fracminhash = np.abs(fracminhash[indices]-jaccard[indices])/jaccard[indices]\n",
    "print(\"Error: %.1f+-%.1f%%\" % (np.mean(error_minhash)*100, np.std(error_minhash)*100))\n",
    "print(\"Error FracMinHash: %.1f+-%.1f%%\" % (np.mean(error_fracminhash)*100, np.std(error_fracminhash)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bfdd5a-e840-4221-a79e-3322d30fa36c",
   "metadata": {},
   "source": [
    "How does the scaling factor influence the accuracy and runtime?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254efe84-7328-4269-ae1f-1cce94f29860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate.plots import plot_fracminhash\n",
    "scaling_factors = np.linspace(0, .5, 10)\n",
    "runtimes, errors = [], []\n",
    "for scaling_factor in scaling_factors:\n",
    "    time, mem, res = track_memory_and_runtime_similarity(\"build/source/fracminhash0\", scaling_factor)\n",
    "    runtimes.append(time)\n",
    "    errors.append(np.mean(np.abs(res[indices]-jaccard[indices])/jaccard[indices])*100)\n",
    "plot_fracminhash(errors, runtimes, scaling_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2980e4cd-8109-4af7-8fe5-f856a33a6b0f",
   "metadata": {},
   "source": [
    "## Take Aways\n",
    "\n",
    "- Certain tasks for massive data as in molecular biology require data sketching.\n",
    "- With a bit of randomness, measures for distinct elements in a set become tractable for big data.\n",
    "- Especially, relying proximity measures that require pairwise comparisons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
